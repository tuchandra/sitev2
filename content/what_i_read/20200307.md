+++
title = "What I read this week (March 1 - March 7)"
date = "2020-03-07"
categories = ["what I read"]
draft = false
+++

Hello! This week's reading comes largely from /r/MachineLearning and a couple other sources.


<!--more-->


# [The Morning Paper](https://blog.acolyer.org/) articles
**Author**: Adrian Colyer

I read three articles from The Morning Paper this week:

 * [Meaningful Availability](https://blog.acolyer.org/2020/02/26/meaningful-availability/) is about the Google G Suite team's search for a *meaningful* availability metric: one that reflects what their users experience while also being useful to their engineers. The post describes the path from "proportion uptime" (my first instinct when thinking about availability) to a more informative metric.
 * [Gandalf: an intelligent, end-to-end analytics service for safe deployment in cloud-scale infrastructure](https://blog.acolyer.org/2020/02/28/microsoft-gandalf/) presents a system built by Microsoft, called Gandalf, which is the software deployment monitor that Azure has used for (at least) a year and a half. The write-up describes how Gandalf improved deployment times, and (my favorite) how teams built trust in it as it detected complex failures that experts missed. Gandalf ingests a large amount of data and passes them through fast (~5 minutes) and slow (~hours) paths to detect defects. The algorithms used are a combination of anomaly detection, correlation analysis, and a decision engine.
 * [Firecracker: lightweight virtualization for serverless applications](https://blog.acolyer.org/2020/03/02/firecracker/) is by far the most systems-heavy of these papers, which describes the virtual machine monitor behind AWS Lambda. The write-up explains some of the typical problems with containers, language VMs, and virtualization, then how Firecracker solves them.

---

[Transformers are Graph Neural Networks](https://graphdeeplearning.github.io/post/transformers-are-gnns/) is a blog post by Chaitanya Joshi in the [NTU Graph Deep Learning Lab](https://graphdeeplearning.github.io/). This establishes links from the [Transformer architecture](https://arxiv.org/abs/1706.03762), used in [GPT-2](https://openai.com/blog/better-language-models/), [BERT](https://www.blog.google/products/search/search-language-understanding-bert/), and more, and [Graph Neural Networks](https://graphdeeplearning.github.io/project/spatial-convnets/). Broadly speaking, Transformers are GNNs with multi-headed attention as the "neighborhood aggregation function" which treats sentences as neighborhoods and uses attention to know which words are "important." 

By establishing links between the two architectures, the author can focus on the question of what they can learn from each other. One question is if fully connected graphs are the best format to use, since they make learning long-term dependencies difficult. From NLP, we have ideas like sparse or adaptive attention, recurrence or compression, or locality sensitive hashing. From GNNs, we might have the idea of binary partitioning to sparsify graphs.

This is a good post, though rather out of my depth. I appreciate the tremendous amount of work which clearly went into this.

---


[Attention and Augmented RNNs](https://distill.pub/2016/augmented-rnns/), by Chris Olah and Shan Carter, is a [Distill publication](https://distill.pub/) about the attention mechanism in RNNs at a higher, more conceptual level. They break down four interesting research applications of attention: neural turing machines, attentional interfaces, adaptive computation time, and neural programmers. One way of thinking about attention is that it "takes every direction at a fork, and then merges the paths back together"---it allows a model to spread out how much it cares about different parts of an input. 

This article was a great introduction to what I previously understood very little of. I don't ever see myself going into machine learning research, but reading about new advances in the field is valuable to me. I'm also a *huge* fan of Distill for [pushing the boundaries](https://distill.pub/about/) of ML articles by devoting themselves to exceptional communication. This would have been impossible for me to get started with had I tried reading the original papers.

---

