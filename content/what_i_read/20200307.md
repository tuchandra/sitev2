+++
title = "What I read this week (March 1 - March 7)"
date = "2020-03-07"
categories = ["what I read"]
draft = false
+++

Hello! This week's reading comes largely from /r/MachineLearning and a couple other sources.


<!--more-->

## [Transformers are Graph Neural Networks](https://graphdeeplearning.github.io/post/transformers-are-gnns/)
**Author**: Chaitanya Joshi

**How I found this**: from /r/MachineLearning

This blog post establishes links from the [Transformer architecture](https://arxiv.org/abs/1706.03762), used in [GPT-2](https://openai.com/blog/better-language-models/), [BERT](https://www.blog.google/products/search/search-language-understanding-bert/), and more, and [Graph Neural Networks](https://graphdeeplearning.github.io/project/spatial-convnets/).  


[Attention and Augmented RNNs](https://distill.pub/2016/augmented-rnns/), by Chris Olah and Shan Carter, is a [Distill publication](https://distill.pub/) about the attention mechanism in RNNs at a higher, more conceptual level. They break down four interesting research applications of attention: neural turing machines, attentional interfaces, adaptive computation time, and neural programmers. One way of thinking about attention is that it "takes every direction at a fork, and then merges the paths back together"---it allows a model to spread out how much it cares about different parts of an input. 

This article was a great introduction to what I previously understood very little of. I don't ever see myself going into machine learning research, but reading about new advances in the field is valuable to me. I'm also a *huge* fan of Distill for [pushing the boundaries](https://distill.pub/about/) of ML articles by devoting themselves to exceptional communication. This would have been impossible for me to get started with had I tried reading the original papers.

---

