+++
title = "What I read this week (March 1 - March 7)"
date = "2020-03-07"
categories = ["what I read"]
draft = false
+++

Hello! This week's reading comes largely from /r/MachineLearning and a couple other sources.


<!--more-->

[Transformers are Graph Neural Networks](https://graphdeeplearning.github.io/post/transformers-are-gnns/) is a blog post by Chaitanya Joshi in the [NTU Graph Deep Learning Lab](https://graphdeeplearning.github.io/). This establishes links from the [Transformer architecture](https://arxiv.org/abs/1706.03762), used in [GPT-2](https://openai.com/blog/better-language-models/), [BERT](https://www.blog.google/products/search/search-language-understanding-bert/), and more, and [Graph Neural Networks](https://graphdeeplearning.github.io/project/spatial-convnets/). Broadly speaking, Transformers are GNNs with multi-headed attention as the "neighborhood aggregation function" which treats sentences as neighborhoods and uses attention to know which words are "important." 

By establishing links between the two architectures, the author can focus on the question of what they can learn from each other. One question is if fully connected graphs are the best format to use, since they make learning long-term dependencies difficult. From NLP, we have ideas like sparse or adaptive attention, recurrence or compression, or locality sensitive hashing. From GNNs, we might have the idea of binary partitioning to sparsify graphs.

This is a good post, though rather out of my depth. I appreciate the tremendous amount of work which clearly went into this.

---


[Attention and Augmented RNNs](https://distill.pub/2016/augmented-rnns/), by Chris Olah and Shan Carter, is a [Distill publication](https://distill.pub/) about the attention mechanism in RNNs at a higher, more conceptual level. They break down four interesting research applications of attention: neural turing machines, attentional interfaces, adaptive computation time, and neural programmers. One way of thinking about attention is that it "takes every direction at a fork, and then merges the paths back together"---it allows a model to spread out how much it cares about different parts of an input. 

This article was a great introduction to what I previously understood very little of. I don't ever see myself going into machine learning research, but reading about new advances in the field is valuable to me. I'm also a *huge* fan of Distill for [pushing the boundaries](https://distill.pub/about/) of ML articles by devoting themselves to exceptional communication. This would have been impossible for me to get started with had I tried reading the original papers.

---

