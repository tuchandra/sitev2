+++
title = "What I read this week (January 26 - February 1)"
date = "2020-02-01"
categories = ["what I read"]
draft = false
+++

Articles I read this week, including about Python 3.9 and TODO. <!--more-->

## [Python 3.9 Compatibility Channges](https://tirkarthi.github.io/programming/2020/01/27/python-39-changes.html/)
**Author**: Karthikeyan Singaravelan

**How I found this**: /r/programming

**Summary**: this is a post about the backwards-incompatible changes being made in Python 3.9. The idea is that a lot of `DeprecationWarning`s will finally be enforced, but this is predictably going to break lots of packages. This led to a thread on the [python-dev mailing list](https://mail.python.org/archives/list/python-dev@python.org/thread/EYLXCGGJOUMZSE5X35ILW3UNTJM3MCRE/) asking for some of these to be postponed to Python 3.10, because the burden of supporting both 2.7 and 3.9 at once is too high.

**My thoughts**: this is tough. On one hand, these deprecation warnings have been emitted since 2014 (Python 3.4), and saying "update your code in the next half decade" is IMO a reasonable thing to ask of your users. On the other, library maintainers often work in their spare time, are largely volunteers, and more generally have little obligation to do *any* of this work.

I'm lucky to be in a position at work where I can generally use new technologies. Because data science is the research arm of Nielsen, we typically have agency to use whatever versions of languages and libraries we want. For my current position, a solo research project, this means I regularly upgrade my libraries as new versions are released. I recognize that this is not at all the norm.

## [The real reason tech companies want regulation](https://www.exponentialview.co/p/-the-real-reason-tech-companies-want)
**Author**: Azeem Azhar

**How I found this**: ?

**Summary**: the leaders of tech companies have started straight up telling Congress (and other government bodies) that they need to be regulated. Sundar Pichai is on record saying "there is no question in my mind that artificial intelligence needs to be regulated." And more and more, tech leaders know that regulation is coming (as it needs to!).

But ... regulation favors large companies. To large companies with large legal teams, regulation is a headache; to new players trying to enter a new space, regulation may stop them entirely.

And ... the size that the largest companies have reached, broadly speaking, is too big for smaller players to compete with. Despite competition to Google only being a click away, no reasonable person would use any other search engine (besides [DuckDuckGo](https://duckduckgo.com/), which I use). While their size already discourages competition, regulation will discourage it further.

**Thoughts**: Ben Thompson wrote about this in [The End of the Beginning](https://stratechery.com/2020/the-end-of-the-beginning/) by suggesting that the biggest tech players today may remain that way, and that the era of small companies disrupting larger ones *in their primary business area* is over. That's not to say that this is the end of progress, or that it'll even slow—rather, the established players will remain that way (much like GM, Ford, and Chrysler in the 20th century).

In fact, Ben writes about tech and monopoly all the time. Stratechery has become the most interesting thing I read each morning because his analyses are so well-argued. 

That regulation is coming is almost certain—the current hot topic is facial recognition, but last year we heard all about Warren talking about the big companies needing to be broken up, and we've seen lots of other discussions pop up. Cynically, the tech giants calling for it sounds like they're just trying to influence the inevitable.

## [If data isn't truth, then what is it?](https://www.research-live.com/article/opinion/if-data-isnt-truth-then-what-is-it/id/5064172)

**Author**: Mihajlo Popesku

**How I found this**: shared in my company's Slack

**Summary**: the idea of "data being truth" is, in many cases, simply false. In e.g., the case of social media, treating data as "evidence" is misleading; they're a social construct, created first by what is actually true, but then distorted by layers of human and algorithmic decision making. The author calls for accepting a more probabilistic view of the world, instead of treating data as hard truth.

**My thoughts**: this is absolutely true, and validates a lot of the work I've been doing over the past half year. If we collect data on human behavior, then we have to accept that humans behave in unpredictable and irrational ways, based on emotions, social pressure, unconscious bias, and much more. Unpacking all of this requires interdisciplinary work that goes well beyond pure "data science" into sociology, behavioral science, and much more.

This speaks more broadly about why I'm worried about Nielsen's strategy of branding itself as the "one media truth." (Reminder: my opinions are my own.) Our data is not truth, nor is anyone else's; we use high-quality methodology, but we'll never arrive at the "truth" of how many people are watching a given program.

I believe that positioning ourselves this way will eventually back us into a corner, where we *have* to accept uncertainty in our predictions (as we start doing more probabilistic modeling), and convince clients to do the same, but where we can't, because we've called ourselves the "truth" for so long.

## Other articles
[Things I believe](https://gist.github.com/stettix/5bb2d99e50fdbbd15dd9622837d14e2b) by Jan Stette is a list of principles he's developed throughout his career. This is a remarkably good collection of one-liners. This is definitely something I'll save for later.

[Your first 90 days as CTO or VPE](https://lethain.com/first-ninety-days-cto-vpe/) by Will Larson is also fantastic. Having recently started a role as the CTO of Calm, Will spoke with many others to write this piece. This gives a great deal of insight into the roles of CTOs / VPEs; it's really interesting to hear about how *specifically* their roles influence the culture and technical direction of a company. Sections on goals and directions, hiring, and technology were the most informative for me.

[How we retired Python 2 and embraced developer happiness](https://engineering.linkedin.com/blog/2020/how-we-retired-python-2-and-improved-developer-happiness) by Barry Warsaw is a story about LinkedIn's migration from Python 2 to Python 3. Loosely speaking, they started with foundational tools and libraries, migrated those (both out of necessity and to build confidence), then migrated the rest. Once they dropped Python 2 support, they were able to start taking advantage of newer features of Python 3.

[Pay Fair](https://increment.com/teams/pay-fair/) by Lara Hogan is an "introduction to correcting—and preventing—compensation inequality." Of note are a couple of strategies for managers and compensation teams to use to prevent companies from paying minoritized groups more. But honestly, the thing I'll remember most is the use of the term "minoritized groups," which "places the emphasis on the power struggle, and on the systemic issues at play." Likewise, when discussing compensation, the author is careful to write "we compensate people of color 10 percent less than white people," rather than "they earn 10 percent less," because the former puts the responsibility on the company for underpaying employees, while the latter moves responsibility to the individual. Language matters.

[How I write backends](https://github.com/fpereiro/backendlore) by Federico Pereiro is an incredibly detailed look at how he writes server-side code for web apps. There's so much I don't get here—tools I haven't heard of, concepts I don't understand—but it's obvious this is really thorough. I appreciate resources like this a lot.




