+++
title = "What I read this week (January 26 - February 1)"
date = "2020-02-01"
categories = ["what I read"]
draft = false
+++

Articles I read this week, including about Python 3.9 and TODO. <!--more-->

## [Python 3.9 Compatibility Channges](https://tirkarthi.github.io/programming/2020/01/27/python-39-changes.html/)
**Author**: Karthikeyan Singaravelan

**How I found this**: /r/programming

**Summary**: this is a post about the backwards-incompatible changes being made in Python 3.9. The idea is that a lot of `DeprecationWarning`s will finally be enforced, but this is predictably going to break lots of packages. This led to a thread on the [python-dev mailing list](https://mail.python.org/archives/list/python-dev@python.org/thread/EYLXCGGJOUMZSE5X35ILW3UNTJM3MCRE/) asking for some of these to be postponed to Python 3.10, because the burden of supporting both 2.7 and 3.9 at once is too high.

**My thoughts**: this is tough. On one hand, these deprecation warnings have been emitted since 2014 (Python 3.4), and saying "update your code in the next half decade" is IMO a reasonable thing to ask of your users. On the other, library maintainers often work in their spare time, are largely volunteers, and more generally have little obligation to do *any* of this work.

I'm lucky to be in a position at work where I can generally use new technologies. Because data science is the research arm of Nielsen, we typically have agency to use whatever versions of languages and libraries we want. For my current position, a solo research project, this means I regularly upgrade my libraries as new versions are released. I recognize that this is not at all the norm.

## [The real reason tech companies want regulation](https://www.exponentialview.co/p/-the-real-reason-tech-companies-want)
**Author**: Azeem Azhar

**How I found this**: ?

**Summary**: the leaders of tech companies have started straight up telling Congress (and other government bodies) that they need to be regulated. Sundar Pichai is on record saying "there is no question in my mind that artificial intelligence needs to be regulated." And more and more, tech leaders know that regulation is coming (as it needs to!).

But ... regulation favors large companies. To large companies with large legal teams, regulation is a headache; to new players trying to enter a new space, regulation may stop them entirely.

And ... the size that the largest companies have reached, broadly speaking, is too big for smaller players to compete with. Despite competition to Google only being a click away, no reasonable person would use any other search engine (besides [DuckDuckGo](https://duckduckgo.com/), which I use). While their size already discourages competition, regulation will discourage it further.

**Thoughts**: Ben Thompson wrote about this in [The End of the Beginning](https://stratechery.com/2020/the-end-of-the-beginning/) by suggesting that the biggest tech players today may remain that way, and that the era of small companies disrupting larger ones *in their primary business area* is over. That's not to say that this is the end of progress, or that it'll even slow—rather, the established players will remain that way (much like GM, Ford, and Chrysler in the 20th century).

In fact, Ben writes about tech and monopoly all the time. Stratechery has become the most interesting thing I read each morning because his analyses are so well-argued. 

That regulation is coming is almost certain—the current hot topic is facial recognition, but last year we heard all about Warren talking about the big companies needing to be broken up, and we've seen lots of other discussions pop up. Cynically, the tech giants calling for it sounds like they're just trying to influence the inevitable.

## [If data isn't truth, then what is it?](https://www.research-live.com/article/opinion/if-data-isnt-truth-then-what-is-it/id/5064172)

**Author**: Mihajlo Popesku

**How I found this**: shared in my company's Slack

**Summary**: the idea of "data being truth" is, in many cases, simply false. In e.g., the case of social media, treating data as "evidence" is misleading; they're a social construct, created first by what is actually true, but then distorted by layers of human and algorithmic decision making. The author calls for accepting a more probabilistic view of the world, instead of treating data as hard truth.

**My thoughts**: this is absolutely true, and validates a lot of the work I've been doing over the past half year. If we collect data on human behavior, then we have to accept that humans behave in unpredictable and irrational ways, based on emotions, social pressure, unconscious bias, and much more. Unpacking all of this requires interdisciplinary work that goes well beyond pure "data science" into sociology, behavioral science, and much more.

This speaks more broadly about why I'm worried about Nielsen's strategy of branding itself as the "one media truth." (Reminder: my opinions are my own.) Our data is not truth, nor is anyone else's; we use high-quality methodology, but we'll never arrive at the "truth" of how many people are watching a given program.

I believe that positioning ourselves this way will eventually back us into a corner, where we *have* to accept uncertainty in our predictions (as we start doing more probabilistic modeling), and convince clients to do the same, but where we can't, because we've called ourselves the "truth" for so long.

