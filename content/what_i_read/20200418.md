+++
title = "What I read this week (April 12 - April 18)"
date = 2020-04-18
categories = ["what I read"]
draft = false
+++

TODO

<!--more-->

## [Leading with the Unknowns in COVID-19 Models](https://blogs.scientificamerican.com/observations/leading-with-the-unknowns-in-covid-19-models/)
**Author**: Dr. Jessica Hullman, in *Scientific American*

I found this from Twitter and it caught my eye; I've read one of the [papers]({{< ref "papers/bayesian_cognition_viz_kim.md"> }}) out of Dr. Hullman's lab before about visualizing uncertainty, so I was excited to see this pop up.

Hullman's thesis is that there are lots of sources of uncertainty implicit in all of the COVID-19 models being circulated:

> What worries me as an expert in reasoning under uncertainty is a more difficult type of uncertainty: the uncertainty that arises from the many unknowns underlying COVID-19 data and models. We can’t easily quantify this uncertainty, and it is easy to overlook, since it is not conveyed by model estimates alone.

The most obvious example is that for a long time, the number of cases was mostly a function of testing capacity. [This is why](https://www.nytimes.com/2020/03/04/us/coronavirus-in-washington-state.html) Washington led the nation in cases for a couple of weeks. A larger number of cases, then, does not necessarily equate to a larger risk.

Other sources of uncertainty, Hullman argues, come from assumptions made about disease transmission or from the unpredictability of human behavior. Every model accounts for these differently, and these assumptions rarely make it into public-facing results. It's important to recognize that these models are based on limited information, and that more reliable data will emerge over time.

> Clear presentation of uncertainty can make model estimates seem less reassuring, but can prevent people from blaming the forecaster or the scientific enterprise itself, when, as we should expect, the model is wrong. Trading public trust in science in the future is not worth feeling more assured in the short term, no matter how much we seek to eliminate uncertainty.  

**My thoughts**: I've seen similar ideas from various folks in data science & statistical modeling Twitter circles. It's clear that a lot of the nuance behind the COVID-19 forecasts is being lost, and it leads to interesting questions about how to best communicate information.

And predictably so! It's much easier to remember "Chicago was supposed to peak on Saturday" than remember "... given that social distancing continues until the end of May," and all the other assumptions implicit in the model (which model?) that produced it.

This raises lots of interesting questions. How do we best communicate the assumptions behind these models? Which assumptions are necessary, and which are acceptable to leave out (bearing in mind the space constraints of things like press releases, headlines, and tweets)? How do the answers here change when communicating to different audiences?

Models are not predictions meant to be right or wrong, either. There are feedback loops implicit in how they are used: if a governor uses a model to inform statewide policy, and the policy changes people's behavior, then of course the model will be "wrong." I have begun thinking of models as "explorations of dynamics," rather than "predictions" (see: [xkcd 2289](https://xkcd.com/2289/)).


## [Just a tweet by Richard McElreath](https://twitter.com/rlmcelreath/status/1249640607250776066)

> People who make model-based forecasts don't (usually) think it is possible to get the model right, in a situation as realistic and complex as an epidemic. So we study many models and try to understand the structural risks of the situation that are common across models. Forecasts are not predictions, but ways to influence policy and behavior. The public and politicians engage with predictions in a very different way, and I think this is a dangerous gap between the way people like me think of complex systems and the way the public thinks of the purpose of forecasts (to predict the future).

I largely agree, but also have some reservations about the idea that forecasts aren't supposed to be "right." 

Public health experts and researchers are able to unpack all the nuance behind the use of the word "right," and indeed I regularly speak at work about how "all our models are wrong." I'm confident in doing so when my audience is other data scientists, but this isn't the sort of thing that we can say to clients (yet). 

I suspect that the public may hear "the model isn't supposed to be right" as "the model is useless." McElreath gets at this in his last sentence when he speaks of forecasts as predicting the future. Perhaps we might reframe this by saying that modeling the numbers isn't as important as modeling the mechanisms of the spread or exploring what different mechanisms (and interventions) might mean.


## [Another Twitter thread by Carl Bergstrom on visualization](https://twitter.com/CT_Bergstrom/status/1249930293928030209)

Bergstrom writes:

> 1. When plotting epidemic curves or death totals, should we divide by population size? Here on twitter this question has generated a lot more heat than light. 
> 
> The answer is a bit subtle and so while I’ve tweeted about this before I want to address it in more detail.

The answer is complex (and makes me annoyed at Twitter's platform for being awful at conveying anything with nuance). One thing that stands out, though, is that if you divide by population size, then you should start with a constant *fraction* of the population, not a constant *number* of cases. The thread is interesting otherwise, too.



