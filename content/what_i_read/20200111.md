+++
title = "What I read this week (January 5 - 11)"
date = "2020-01-11"
categories = ["what I read"]
draft = false
+++

Articles I read this week, including predictions for 2020 and TODO. <!--more-->

## [Top minds in machine learning predict where AI is going in 2020](https://venturebeat.com/2020/01/02/top-minds-in-machine-learning-predict-where-ai-is-going-in-2020/)
**Author**: Khari Johnson

**How I found this**: from /r/machinelearning

**Summary**: this is an interview with "PyTorch creator Soumith Chintala, University of California professor Celeste Kidd, Google AI chief Jeff Dean, Nvidia director of machine learning research Anima Anandkumar, and IBM Research director Dario Gil."

 * Chintala: thinks that we haven't seen anything since Transformer (2017). He predicts that the community will emphasize things besides accuracy, like explainability and "how AI can better reflect the kind of society people want to build."
 * Kidd: thinks it's likely that there'll be an increased connection between what you already know and what you want to learn next. This was one of the subjects in her [NeurIPS talk]({{< ref "/posts/talk_kidd_neurips.md" >}}), where she talked about the zone between people's previous interests and what's suprising to them as the place where learning is most likely to happen. She also thinks that the "black box" term is meaningless, because brains, too, are black boxes. "In 2020, she wants to see increased awareness of the real-life implications of tech tools and technical decisions and **a rejection of the idea that the makers of tools arenâ€™t responsible for what people do with them.**"
 * Dean: expects to see progress in multitask and multimodal learning, and pointed to the continued growth of language models. He wants to see more on robust models in favor of SOTA advances.
 * Anandkumar: expects progress in "iterative algorithms, self-supervision, and self-training methods," basically thinking the concept of training a feed-forward net once is where robustness issues come from, and that iterative algorithms can help with this. "Policymakers, individuals, and the AI community will also need to grapple with issues of representation and the challenge of ensuring data sets used to train models account for different groups of people."
 * Gil: predicts more progress to efficient training "with reduced-precision architectures" (what?). He hopes that the field will move beyond accuracy to "building trusted systems," and also that it's important to make AI more accessible to normal data scientists or software engineers. Issues with data fairness and integrity will continue to get more attention.

**Thoughts**: this is a well-written article with a ton of different ideas. I think one of the recurring themes is a shift away from accuracy and SOTA performance in favor of robustness and explainability. Kidd's thoughts were the most interesting to me, both that brains are black boxes, too, and the idea of makers being responsible for the tools that they develop. I'm interested to think more about these throughout the year.


