+++
title = "Why I'm interested in HCI"
date = "2019-10-28"
draft = false
categories = ["general"]
+++

A couple nights ago, I was at a bar talking to a grad student friend and we started having a conversation about HCI and research interests. This post captures some of my thoughts after reflecting for a couple of days.

<!--more-->

## What I thought HCI was
I took one HCI class at Northwestern, called simply "Human Computer Interaction." This was in winter 2016, so it's possible that the class has changed, but I remember the class being a mixture of human-centric design and front end development. Assignments (that I can remember) included:

 * reading some of Don Norman's [The Design of Everyday Things](https://en.wikipedia.org/wiki/The_Design_of_Everyday_Things)
 * programming a calculator interface in HTML/CSS/JS
 * critiquing the design of everyday objects (e.g., doorknobs)
 * information and data visualization
 * programming some kind of web app in HTML/CSS/JS as a final project

After taking this course, I thought HCI == front end interface development. I thought that the tools of HCI were CSS and jQuery, and that researchers in HCI spent their days thinking about how to make websites better. (Ironically, in addition to warping my perception of HCI, this class *also* made me think front end developers tweaked stylesheets and tried to get buttons to change color all day! Having a React developer as a roommate quickly showed me how wrong that is a couple years later.)

To Northwestern's credit, designing undergrad courses is hard, especially when you're trying to accommodate a CS major whose size [tripled in five years](https://news.northwestern.edu/stories/2016/06/major-expansion-in-computer-science). I learned a lot from the course, but there's only so much you can do when you have to come up with assignments that are feasible to score for 150+ students (e.g., favor group web apps over individual paper summaries).


## How that changed
As I continued at NU, I took more classes that were HCI-adjacent. Two classes with [Brent Hecht](http://www.brenthecht.com) (who describes his research at the intersection of HCI, social computing, and geography) titled "Algorithms and Society" and "Spatial Data Science & Computing" helped me to see what HCI *actually* was. A research seminar with Josiah Hester on the [Internet of Things](https://www.mccormick.northwestern.edu/electrical-computer/courses/descriptions/395-495-42.html) helped me to take an HCI perspective on research papers. Over time, I started seeing how HCI is such a unique field, because it touches all other disciplines of computing.

And over the past year, I've been able to take more time to reflect on HCI and engage with some of the literature. With the help of my friend and colleage [Judah Newman](https://judahgnewman.com/), I've been able to engage with papers, talk about research directions, and discuss open problems. I'm now more able to engage with research than I was in school, and it feels great.


## What I'm thinking about the future
I took a few minutes to think about research questions that I've thought about or read papers on. This is a brainstorm, so I haven't filtered this list at all, and there's certainly existing work in all of these areas. But here we are:

 * How much do "feed algorithms" (e.g., Facebook news feed, Twitter feed, Reddit homepage) influence what kinds of content people are exposed to? How much can users control those algorithms? What effects do those algorithms have on users' emotions?
 * If I'm a malicious actor, what are the most effective strategies for spreading fake news or misinformation? How do we stop it? How do we even study this ethically?
 * What is the extent of filter bubbles on Reddit? How much does self-selecting subreddits limit the information you are exposed to?
 * How do we build tools for content moderation and fact checking at scale?
 * Where is the line between censorship and free speech on social media? To what extent are users okay with different degrees of moderation on various platforms?
 * To what extent do humans mistake text generated by SOTA language models (e.g., GPT-2) as human writing?
 * How do we build tools for exploring biases and inconsistencies in machine learning models? For understanding the output of machine learning models?
 * How do we rethink the machine learning workflow so that fairness and interpretability are integrated into the lifecycle, instead of being an afterthought once your model is "done"?
 * How do we account or correct for systemic biases in data sources?
 * How do we create visualizations that avoid priming our audience in a certain way? Is that even possible?

I came up with this list in 30 minutes while watching [Smash Summit 2](https://smash.gg/tournament/smash-ultimate-summit-2/events)--that is to say, I wasn't very focused and I'm sure there are important questions that I've left off. But I'm starting to imagine what I could do as a researcher in this area, and the possibilities seem endless.

I think that a lot of the answers to the biggest questions in CS (and data science & machine learning where I currently work) are going to come from HCI. My grad student friend said that a lot of people have said exactly that over the year that he's been in school. That's both reassuring and exciting.
